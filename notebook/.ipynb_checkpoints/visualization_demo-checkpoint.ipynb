{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d07c0497-311d-481c-bcaf-dc6708458d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "from IPython.display import Image as IImage\n",
    "\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from scenarionet.common_utils import read_scenario, read_dataset_summary\n",
    "\n",
    "from metadrive.policy.replay_policy import ReplayEgoCarPolicy\n",
    "from metadrive.envs.scenario_env import ScenarioOnlineEnv\n",
    "from metadrive.scenario.scenario_description import ScenarioDescription\n",
    "\n",
    "from unitraj.datasets.base_dataset import BaseDataset\n",
    "\n",
    "sys.path.append('../') # for import of local dataset and model modules\n",
    "\n",
    "from datasets.basic_dataset import BasicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830d49a5-cba7-44eb-86ec-dcb902ba78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title constants; change as needed\n",
    "OUTPUT_DIR = \"output\"  # where visualizations are saved relative to this file\n",
    "CKPT = None\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CUR_TIME_IDX = 20  # don't modify this\n",
    "\n",
    "TRAVERSE_PATH = \"/fs/nexus-projects/pc_driving/datasets/traverse\"\n",
    "\n",
    "TRAIN_DATA_PATH = f\"{TRAVERSE_PATH}/sd_sumo/train\"\n",
    "VAL_DATA_PATH = f\"{TRAVERSE_PATH}/sd_sumo/val\"\n",
    "CACHE_PATH = f\"{TRAVERSE_PATH}/sd_sumo/cache\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c0e09b-1b59-4570-a663-79ba63a5926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create a gif file from a list of images\n",
    "def make_GIF(frames, name=\"demo.gif\"):\n",
    "    print(f\"Generate gif @ {name}...\")\n",
    "    imgs = [frame for frame in frames]\n",
    "    imgs = [Image.fromarray(img) for img in imgs]\n",
    "    imgs[0].save(name, save_all=True, append_images=imgs[1:], duration=50, loop=0)\n",
    "\n",
    "# creates an environment compatible with the datasets and model predictions\n",
    "def create_env(data_path, threeD_render=False):\n",
    "    return ScenarioOnlineEnv(\n",
    "        {\n",
    "            \"manual_control\": False,\n",
    "            \"reactive_traffic\": False,\n",
    "            \"use_render\": threeD_render,\n",
    "            \"agent_policy\": ReplayEgoCarPolicy,\n",
    "            \"data_directory\": data_path,  # use nuscenes data\n",
    "            \"num_scenarios\": 10,  # load 10 scenarios\n",
    "        }\n",
    "    )\n",
    "\n",
    "# reads scenarionet data\n",
    "def get_scenario_list(data_path):\n",
    "    scenario_list = []\n",
    "    _, summary_list, mapping = read_dataset_summary(data_path)\n",
    "    data_list = list(summary_list)\n",
    "    # data_path, mapping, list(summary_list), dataset_name\n",
    "\n",
    "    for cnt, file_name in enumerate(data_list):\n",
    "        scenario = read_scenario(data_path, mapping, file_name)\n",
    "        scenario_list.append(scenario)\n",
    "\n",
    "    return scenario_list\n",
    "\n",
    "# helper function for sending dictionary samples to devices\n",
    "def send_data_to_device(data, device):\n",
    "    if isinstance(data, dict):\n",
    "        for key in data.keys():\n",
    "            data[key] = send_data_to_device(data[key], device)\n",
    "    elif isinstance(data, list):\n",
    "        for i in range(len(data)):\n",
    "            data[i] = send_data_to_device(data[i], device)\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        data = data.to(device)\n",
    "\n",
    "    return data\n",
    "\n",
    "# replaces the GT trajectory after CUR_TIME_IDX to the predicted trajectory\n",
    "def modify_scenario_with_prediction(scenario, processed_scenario, prediction):\n",
    "    scenario = copy.deepcopy(scenario)\n",
    "    # sdc_id = scenario[ScenarioDescription.METADATA][ScenarioDescription.SDC_ID]\n",
    "    sdc_id = 'ego'\n",
    "    ego_idx = processed_scenario[\"input_dict\"][\"center_objects_id\"].index(sdc_id)\n",
    "    # ego_idx = 0\n",
    "    ego_prediction = prediction[ego_idx]\n",
    "    timesteps = ego_prediction.shape[0]\n",
    "\n",
    "    scenario[ScenarioDescription.TRACKS][sdc_id][\"state\"][\"position\"][-timesteps:, 0:2] = ego_prediction[:, 0:2]\n",
    "    scenario[ScenarioDescription.TRACKS][sdc_id][\"state\"][\"heading\"][-timesteps:, 0] = ego_prediction[:, 2] # + start_heading\n",
    "    return scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "105ff578-035e-4db2-bf68-9d7793e5211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main render function definition; \n",
    "def render(data_path: str, dataset: BaseDataset, model: torch.nn.Module = None, idxs: int = 0, threeD_render=False):\n",
    "    # threeD_render=False # turn on this to enable 3D render. It only works when you have a screen and not running on Colab.\n",
    "    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # Hide the pygame window\n",
    "\n",
    "    env = create_env(data_path, threeD_render=threeD_render)\n",
    "    scenario_list = get_scenario_list(data_path)\n",
    "    output_files = []\n",
    "    \n",
    "    try:\n",
    "        for i in idxs:\n",
    "            scenario_to_render_gt = scenario_list[i]\n",
    "            processed_scenario = dataset.preprocess(scenario_to_render_gt)\n",
    "            processed_scenario = dataset.process(processed_scenario)\n",
    "            processed_scenario = dataset.postprocess(processed_scenario)\n",
    "            processed_scenario = dataset.collate_fn(processed_scenario)\n",
    "    \n",
    "            env.set_scenario(scenario_data=scenario_to_render_gt)\n",
    "            print(\"\\nSimulate Scenario (GT): {}\".format(i))\n",
    "            o, _ = env.reset()\n",
    "            # env.agent_manager.agent_policy.set_prediction(action)\n",
    "            # ego_idx = processed_scenario['input_dict']['center_objects_id'].index(env.engine.traffic_manager.sdc_object_id)\n",
    "            frames = []\n",
    "            for _ in range(1, 100000):\n",
    "                o, r, tm, tc, info = env.step([0.0, 0.0])\n",
    "                frames.append(env.render(mode=\"top_down\", film_size=(4000, 4000), screen_size=(500, 500)))\n",
    "                if tm or tc:\n",
    "                    make_GIF(frames, name=f\"{OUTPUT_DIR}/scenario_{scenario_to_render_gt['id']}_{i}_gt.gif\")\n",
    "                    output_files.append(f\"{OUTPUT_DIR}/scenario_{scenario_to_render_gt['id']}_{i}_gt.gif\")\n",
    "                    break\n",
    "    \n",
    "            if model is not None:\n",
    "                # model prediction here\n",
    "                with torch.no_grad():\n",
    "                    processed_scenario = send_data_to_device(processed_scenario, DEVICE)\n",
    "                    prediction, loss_dict = model(processed_scenario)\n",
    "        \n",
    "                pred_trajs = prediction[\"predicted_trajectory\"]\n",
    "                num_center_objs, k, num_steps, _ = pred_trajs.shape\n",
    "                pred_trajs_mean, pred_vel = pred_trajs[:, :, :, 0:2], pred_trajs[:, :, :, 5:7]\n",
    "                highest_prob_k = prediction[\"predicted_probability\"].argmax(dim=-1)\n",
    "                highest_prob_pos = pred_trajs_mean[torch.arange(num_center_objs), highest_prob_k]\n",
    "                highest_prob_pos = highest_prob_pos[..., [1, 0]] # TODO: delete\n",
    "                # highest_prob_vel = pred_vel[torch.arange(num_center_objs), highest_prob_k]\n",
    "                starting_pos = torch.Tensor(scenario_to_render_gt[ScenarioDescription.TRACKS]['ego']['state']['position'][CUR_TIME_IDX, 0:2]).to(DEVICE)\n",
    "                zeros_pos = torch.zeros(highest_prob_pos.shape[0], 1, highest_prob_pos.shape[-1]).to(DEVICE)\n",
    "                highest_prob_pos_prepended = torch.cat([zeros_pos, highest_prob_pos], dim=1) + starting_pos\n",
    "                highest_prob_vel = torch.diff(highest_prob_pos_prepended, dim=1)\n",
    "                highest_prob_heading = torch.atan2(highest_prob_vel[..., 1], highest_prob_vel[..., 0])\n",
    "                action = torch.cat([highest_prob_pos + starting_pos, highest_prob_heading.unsqueeze(-1)], dim=-1).cpu().numpy()\n",
    "        \n",
    "                scenario_to_render = modify_scenario_with_prediction(scenario_to_render_gt, processed_scenario, action)\n",
    "        \n",
    "                env.set_scenario(scenario_data=scenario_to_render)\n",
    "                print(\"\\nSimulate Scenario (Pred): {}\".format(i))\n",
    "                o, _ = env.reset()\n",
    "                frames = []\n",
    "                for step in range(1, 100000):\n",
    "                    o, r, tm, tc, info = env.step([0.0, 0.0])\n",
    "                    frames.append(env.render(mode=\"top_down\", film_size=(4000, 4000), screen_size=(500, 500)))\n",
    "                    if tm or tc:\n",
    "                        print(f\"Done at {step} total frames\")\n",
    "                        make_GIF(frames, name=f\"{OUTPUT_DIR}/scenario_{scenario_to_render['id']}_{i}_pred.gif\")\n",
    "                        output_files.append(f\"{OUTPUT_DIR}/scenario_{scenario_to_render['id']}_{i}_pred.gif\")\n",
    "                        break\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        raise e\n",
    "        pass\n",
    "        \n",
    "    env.close()\n",
    "    return output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "245f6cec-ac8b-4c35-ac6c-d56f0f317751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Environment: ScenarioOnlineEnv\u001b[0m\n",
      "\u001b[38;20m[INFO] MetaDrive version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n",
      "\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n",
      "\u001b[38;20m[INFO] Horizon (Max steps per agent): None\u001b[0m\n",
      "\u001b[38;20m[INFO] Assets version: 0.4.3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp_name': 'test', 'ckpt_path': None, 'seed': 42, 'debug': True, 'devices': [0], 'load_num_workers': 0, 'train_data_path': ['/fs/nexus-projects/pc_driving/datasets/traverse/sd_sumo/train'], 'val_data_path': ['/fs/nexus-projects/pc_driving/datasets/traverse/sd_sumo/val'], 'cache_path': '/fs/nexus-projects/pc_driving/datasets/traverse/sd_sumo/cache', 'max_data_num': [None], 'starting_frame': [0], 'past_len': 21, 'future_len': 60, 'object_type': ['VEHICLE'], 'line_type': ['lane', 'stop_sign', 'road_edge', 'road_line', 'crosswalk', 'speed_bump'], 'masked_attributes': ['z_axis', 'size'], 'trajectory_sample_interval': 1, 'only_train_on_ego': False, 'center_offset_of_map': [30.0, 0.0], 'use_cache': True, 'overwrite_cache': False, 'store_data_in_memory': False, 'oversampling': False, 'undersampling': False, 'portion': None, 'nuscenes_dataroot': '/mnt/nas3_rcp_enac_u0900_vita_scratch/datasets/Prediction-Dataset/nuscenes/nuscenes_root', 'eval_nuscenes': False, 'eval_waymo': False, 'eval_argoverse2': False, 'max_num_agents': 64, 'map_range': 100, 'max_num_roads': 768, 'max_points_per_lane': 20, 'manually_split_lane': True, 'point_sampled_interval': 1, 'num_points_each_polyline': 20, 'vector_break_dist_thresh': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n",
      "\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 10\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulate Scenario (GT): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Episode ended! Scenario Index: 1 Scenario id: 10_FCD_T001 Reason: arrive_dest.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate gif @ output/scenario_10_FCD_T001_1_0_gt.gif...\n",
      "\n",
      "Simulate Scenario (GT): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/nexus-scratch/lyzheng/miniconda3/envs/pc_driving/lib/python3.9/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/fs/nexus-scratch/lyzheng/miniconda3/envs/pc_driving/lib/python3.9/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/fs/nexus-scratch/lyzheng/miniconda3/envs/pc_driving/lib/python3.9/site-packages/numpy/core/_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "\u001b[38;20m[INFO] Episode ended! Scenario Index: 8 Scenario id: 10_FCD_T001 Reason: arrive_dest.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate gif @ output/scenario_10_FCD_T001_10_1_gt.gif...\n",
      "\n",
      "Simulate Scenario (GT): 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Episode ended! Scenario Index: 2 Scenario id: 10_FCD_T001 Reason: arrive_dest.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate gif @ output/scenario_10_FCD_T001_11_2_gt.gif...\n",
      "\n",
      "Simulate Scenario (GT): 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Episode ended! Scenario Index: 6 Scenario id: 10_FCD_T001 Reason: arrive_dest.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate gif @ output/scenario_10_FCD_T001_2_3_gt.gif...\n"
     ]
    }
   ],
   "source": [
    "config_path = \"./\"\n",
    "config_name = \"config_demo.yaml\"\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(version_base=None, config_path=config_path)\n",
    "cfg = hydra.compose(config_name=config_name)\n",
    "\n",
    "cfg[\"train_data_path\"] = [TRAIN_DATA_PATH]\n",
    "cfg[\"val_data_path\"] = [VAL_DATA_PATH]\n",
    "cfg[\"cache_path\"] = CACHE_PATH\n",
    "\n",
    "# check if cache has already been created\n",
    "if os.path.exists(cfg[\"cache_path\"]) and not os.path.isfile(cfg[\"cache_path\"]) and os.listdir(cfg[\"cache_path\"]):\n",
    "    cfg[\"use_cache\"] = True \n",
    "    cfg[\"overwrite_cache\"] = False\n",
    "    \n",
    "print(cfg)\n",
    "\n",
    "model = None\n",
    "ckpt = None\n",
    "val_set = BasicDataset(cfg, is_validation=True)\n",
    "data_dir = val_set.data_path[0]\n",
    "output_files = render(data_dir, val_set, model, idxs=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d87893-095d-4639-9df9-22bb2486d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np    \n",
    "\n",
    "#Create reader object for the gif\n",
    "gifs = [imageio.get_reader(x) for x in output_files]\n",
    "\n",
    "#If they don't have the same number of frame take the shorter\n",
    "number_of_frames = min([gif.get_length() for gif in gifs])-1\n",
    "\n",
    "#Create writer object\n",
    "new_gif = imageio.get_writer('all_output.gif')\n",
    "\n",
    "for frame_number in range(number_of_frames):\n",
    "    imgs = [gif.get_next_data() for gif in gifs]\n",
    "    # here is the magic\n",
    "    new_image = np.hstack(imgs)\n",
    "    new_gif.append_data(new_image)\n",
    "\n",
    "[gif.close() for gif in gifs]  \n",
    "new_gif.close()\n",
    "\n",
    "# visulization\n",
    "IImage(open(\"all_output.gif\", 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692f658-4b38-4564-b1d3-7d3870f74579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
